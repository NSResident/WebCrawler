Crawling:
    Parse config file
    Only grab links that match starting domain (i.e facebook.com)
    Recover from HTTP 400/500 Errors
    Robots.txt crawl (probably add robotsCrawl after initial crawling is done)
    Crawl relevant subdomains

Parsing:
   Check reverse and leet functions
   Check map keys/values

BruteForce:
    Generate reverse and leet speak of parsed words
    Find form elements
    Notify user whether a login was succesful 
    Find 2 form fields, 1 being password

Questions:
    -Do the keywords have to be in search order
    Robots.txt/subdomain
    -How deep
    -Does it count towards page count 
    -Does it also follow dfs/bfs

