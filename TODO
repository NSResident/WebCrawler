Requester:
    Implement GET
        -Error checks
        -403/404/500 checks
        -Parse url for path
        -Parse response for payload
        -Loop for long messages
        -Make modular
    Implement POST (HOLD)
    WireShark for 300 Error

Crawling:
    Create map of already traversed links (?)
    Robots.txt crawl (probably add robotsCrawl after initial crawling is done)

Parsing:

BruteForce:
    Find form elements
    Notify user whether a login was succesful 

Questions:
    -Do the keywords have to be in search order
    Robots.txt/subdomain
    -How deep
    -Does it count towards page count 
    -Does it also follow dfs/bfs

